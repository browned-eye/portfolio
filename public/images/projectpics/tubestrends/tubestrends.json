[{

        "header": "An ETL to Collect Internet Trends ",
        "image": "tubestrend1.png",
        "type": "image",
        "desc": "In popular media, there is much talk about social networking trends, and “what’s trending” online. For the data mining final project, I team was interested in exploring trends published on social media platforms. <br><br>TubesTrends is an ETL that makes a series of automated OO-ruby scripts make API calls and scrape websites. The automation allows internet trend data to be collected 24/7.<br><br>I tried to use the collected to data because I wanted a way to: <br>1. See aggregated internet trends in real time and over the long-term.<br>2.Compare and contrast what's “trending” on different internet and social media platforms to see if there are similar common themes."

    }, {
        "header": "Data Sources",
        "image": "tubestrend2.png",
        "type": "image",
        "desc": "I collected data from the following places, either by calling an API or through web scraping:<br><br> - <a href='https://dev.twitter.com/rest/reference/get/trends/place'> Twitter Trends API resource, GET trends/place</a> <br>- <a href='http://www.google. com/trends/hottrends'> Google Hot Trends</a><br>- <a href='https://instagram.com/developer/api-console/?hl=en'> Instagram API resource, GET Most Popular</a><br>- <a href='www.yahoo.com'> Yahoo <a/>(trends located on right side of homepage page)<br>- <a href=’https://www.youtube.com/trendsdashboard’> YouTubes Trends dashboard </a>"
    }, {
        "header": "Automated ETL Pipeline",
        "image": "tubestrend3.png",
        "type": "image",
        "desc": "When possible, I tried to make API calls to collect the data, however, only Twitter and Instagram publish their self­ reported trend data via an API. To collect trend data from Google, YouTube, and Yahoo, I had to extract from the website HTML directly. The process of scraping was multilayered owing to the fact that javascript renders the HTML for websites directly to the browser. ­­ Additionally, some of the data I collected appeared on multiple pages of a given web platform. <br><br> To circumvent these complications, we used the Ruby library called Headless and Watir. With <a href='https://github.com/leonid-shevtsov/headless'>Headless</a>, I was able to create an in­shell instance of a FireFox browser, which allowed me to render and parse the website’s html.  Using the <a href='https://github.com/watir/watir'>watir library</a>, I automated the process of clicking a series of links and menu buttons letting us collect trend data from multiple pages on given platform."
    },

    {
        "header": "Data Analysis Questions",
        "image": "tubestrend4.png",
        "type": "image",
        "desc": "After quickly exploring the data, my project partner and I realized that even on the same platform, these sites made no effort to normalize terms, if they happened to trend under different spellings. Having learned of different ways of calculating similarity of two document vectors in class, the team realized that calculating the Jaccard coefficient for each individual ‘trend’ formulation against each other term might reveal which trends were likely to represent the same underlying trend underneath. We settled on a Jaccard similarity of 0.7 or over as expressing with strong confidence that the terms involved represented the same underlying trend. Our final output was: <br> <i>Parent Trend</i>: bayern real <i>Child Trends</i>: ['real madrid', 'real bayern', 'real madrid vs bayern munich', 'bayern munich vs real madrid']. <br><br> I also wrote a map-reduce cosine similarity job to calculate the similarity of trending topics between each social network for each day. Cosine similarity takes into account the length of the document and the frequency of the number of times the terms appears in the document.  Unfortunately, we did not get great results from this. This was due to the great variation in the way that trend phrases appeared within and between web platforms. If I were to do this project again, I would first normalize the terms using Jaccard similarity and then run the cosine similarity job over the normalized trend terms."
    },
     {
        "header": "Lessons Learned",
        "image": "antsfinal.jpg",
        "type": "image",
        "desc":"Using the library <a href='https://github.com/s3fs-fuse/s3fs-fuse'> s3fuse </a>, I was able to write the trends data directly to s3 data bucket rather than write to my aws ec2 server. This minimized the chance for data loss in the event that my ec2 server ever went down and reduced money server costs, as it cheaper to pay for s3 storage space than amazon EBS storage.<br><br>I had also hoped to make data visualizations from the trending data and just blindly started collecting data, without doing enough exploratory data analysis. I assumed that the the data would yield good visualizations. When it came time to create the visualizations, I found it difficult to make visualizations that could answer meaningful questions or insights. <br><br> This project taught me that if you’re trying to make an ETL to support a set of data visualizations, you should collect a small sample of the data and try visualize it first. If that small sample of data yields interesting visualizations, then create the ETL to support those visualizations. In retrospect, this seems like common sense, but sometimes you have suffer through something to learn your lesson. <br><br> Overall, I learned a lot about server-side automation from working on this project. Creating a completely automated process to collect data was time consuming, however, the overall benefit was worth it; was the ETL was up and running, I could 'set it and forget it'."
      }



]
